*Emotion Based Music Player*<br>
*ğŸ¯ Goal*<br>
An emotion-based music player solves key challenges by using emotion detection techniques, such as facial recognition or voice analysis, to automatically curate playlists that match the userâ€™s mood. It adapts to real-time emotional changes, providing a dynamic music experience tailored to the user's feelings.<br>

*ğŸ§µ Dataset*<br>
The dataset used in this project are emotion.npy and label.npy which are typically used to store NumPy arrays, which may contain data such as numerical values, model weights, or feature sets, possibly for your emotion-based music player project..<br>
<br>

*ğŸ§¾ Description*<br>
This project utilizes various python libraries such as streamlit, keras, numpy to create the model,to take the input from user and give the required output<br>
<br>


<br>

*ğŸš€ Models Implemented*<br>
1. Pretrained Deep Learning Model (model.h5):

-->Likely a Convolutional Neural Network (CNN) used for emotion recognition, especially in processing facial and hand landmarks for detecting user emotions.
-->The model is loaded using Keras' load_model function, indicating it's a neural network trained on emotion-labeled data.<br>
2. Mediapipe's Holistic and Hands Models:

-->Mediapipe Holistic: Used for detecting key facial and body landmarks.
-->Mediapipe Hands: Used for detecting hand landmarks to infer gestures that may also be used for emotion recognition.
<br>


*ğŸ“š Libraries Needed*<br>
1. Streamlit
2. Streamlit-webrtc
3. Opencv-python
4. Mediapipe
5. Keras
6. Numpy

<br>


*ğŸ“¢ Conclusion*<br>
The emotion-based music player successfully integrates deep learning and computer vision techniques to create a personalized, emotion-driven music experience. By leveraging facial expression and hand gesture recognition through Mediapipe, combined with a pretrained deep learning model, the system can detect the user's emotional state in real-time. This allows for dynamic music recommendations that adapt to the user's mood, enhancing the listening experience.<br>

The project demonstrates how artificial intelligence can transform user interaction with media, making it more intuitive, personalized, and engaging. With future improvements, such as more advanced emotion recognition and enhanced music recommendations, this system could revolutionize how users interact with digital content, making it more emotionally responsive and contextually aware.<br>

<br>

âœ’ï¸ Your Signature<br>
Nadipudi Shanmukhi satya<br>